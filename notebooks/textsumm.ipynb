{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textsumm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN9oN5bw3xnO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "2404d5ea-4f70-420b-89aa-be898c71ba48"
      },
      "source": [
        "# install nlp\n",
        "!pip install nlp\n",
        "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
        "import pyarrow\n",
        "if int(pyarrow.__version__.split('.')[1]) < 16:\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYkraev_4BA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU9Rb0iTv5Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR) #log only errors and supress warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNkflrsr1NKw",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Used- CNN-DailyMail news articles and highlights dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-JCf6th4lbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = nlp.load_dataset('cnn_dailymail', '3.0.0', split='train[0:10000]') #287000+ samples in the training dataset, everything is cached so once youve loaded the cnn-dm dataset, you can alter the split sizes in no time \n",
        "dataset_valid = nlp.load_dataset('cnn_dailymail', '3.0.0', split='validation[:1000]')#13000+ samples in the validation set, using 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBsf9uyj7HOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6b9043ba-3e2d-46e2-c330-873b27651420"
      },
      "source": [
        "print(dataset_train)\n",
        "print(dataset_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(schema: {'article': 'string', 'highlights': 'string', 'id': 'string'}, num_rows: 10000)\n",
            "Dataset(schema: {'article': 'string', 'highlights': 'string', 'id': 'string'}, num_rows: 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKqNt26R1Y7t",
        "colab_type": "text"
      },
      "source": [
        "## Rouge Metric for Model Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgCr2gCV8nFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "4e30643e-f76a-4788-c3e7-8f16e6b8936e"
      },
      "source": [
        "!pip install rouge_score rouge_score\n",
        "rouge_metric = nlp.load_metric(\"rouge\") #rouge is a very standard metric for seq2seq tasks, especially summarization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.18.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLUq8vHbFF9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "e873e70a-0090-4fc0-da73-e0d219f56cec"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xasFBFuHKcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U9bRSuPGFlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoPG9EFy1gGV",
        "colab_type": "text"
      },
      "source": [
        "## Model to fine tune: BartForConditionalGeneration (seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSpO9GODIBAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eac6ccd3-2e35-456e-c3d7-9e02c9b05d68"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')#bart-large is also available, but bart-base is much smaller and switching to large may or may not be worth the extra computation\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): EncoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): DecoderLayer(\n",
              "          (self_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): SelfAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjG80hwMlTQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 1024\n",
        "MAX_SUMMARY_LEN = 200\n",
        "BATCH_SIZE = 2 #Experimented with sizes of 2, 4, 8, and 16. 2 seems to be optimal\n",
        "EPOCHS = 2 #going over 4 epochs may result in overfitting\n",
        "LEARNING_RATE = 5e-5 #any value from 3e-4 to 5e-5 works reasonably well\n",
        "ADAM_EPS = 1e-8\n",
        "NUM_BEAMS = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6v35QUF9EE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'batch_size': BATCH_SIZE,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 0\n",
        "        }\n",
        "\n",
        "train_loader = DataLoader(dataset_train, **params)\n",
        "valid_loader = DataLoader(dataset_valid, **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l43x9HrsTORG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE, eps=ADAM_EPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a40TdEw1p4y",
        "colab_type": "text"
      },
      "source": [
        "### Train Function\n",
        "Pytorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BntcNcjaGlfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for _,data in enumerate(train_loader, 0):\n",
        "    source = tokenizer.batch_encode_plus(data['article'], max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt', truncation='only_first')\n",
        "    target = tokenizer.batch_encode_plus(data['highlights'], max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt', truncation='only_first')\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "    #\n",
        "    y = target_ids.to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous() #to create labels for target, we need to right shift the target ids\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 #padding tokens should be marked with -100 value so the model can output correct loss\n",
        "    ids = source_ids.to(device, dtype = torch.long)\n",
        "    mask = source_mask.to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "        \n",
        "    if _%100==0:\n",
        "      print(f'Epoch: {epoch}, Loss:  {loss}')\n",
        "        \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ql_KdpP1vfg",
        "colab_type": "text"
      },
      "source": [
        "### Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1VFJK6zVu0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate():\n",
        "  model.eval()\n",
        "  prediction = []\n",
        "  reference = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(valid_loader, 0):\n",
        "        source = tokenizer.batch_encode_plus(data['article'], truncation='only_first', max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = tokenizer.batch_encode_plus(data['highlights'], truncation='only_first', max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt')\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        y = target_ids.to(device, dtype = torch.long)\n",
        "        ids = source_ids.to(device, dtype = torch.long)\n",
        "        mask = source_mask.to(device, dtype = torch.long)\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids = ids,\n",
        "            attention_mask = mask,\n",
        "            max_length=MAX_SUMMARY_LEN, \n",
        "            num_beams=NUM_BEAMS,\n",
        "            repetition_penalty=2.5, \n",
        "            no_repeat_ngram_size=4,\n",
        "            early_stopping=True\n",
        "            )#can experiment with different values of num_beams and penalties\n",
        "        pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "        ref = [tokenizer.decode(r, skip_special_tokens=True, clean_up_tokenization_spaces=True)for r in y]\n",
        "        if(_%100 == 0):\n",
        "          print(f\"{_} batches complete\")\n",
        "\n",
        "        prediction.extend(pred)\n",
        "        reference.extend(ref)\n",
        "  return prediction, reference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRNEQbDO1yNH",
        "colab_type": "text"
      },
      "source": [
        "### Training for 2 epochs (experimented with the number of epochs and ended up with 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUN_q0KR-r_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebb9e0f8-b8d3-4f20-eb47-ba6b319cd02a"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  7.20695686340332\n",
            "Epoch: 0, Loss:  2.081721067428589\n",
            "Epoch: 0, Loss:  3.5891597270965576\n",
            "Epoch: 0, Loss:  2.746739387512207\n",
            "Epoch: 0, Loss:  2.469104528427124\n",
            "Epoch: 0, Loss:  3.4169774055480957\n",
            "Epoch: 0, Loss:  2.6619198322296143\n",
            "Epoch: 0, Loss:  2.9047155380249023\n",
            "Epoch: 0, Loss:  3.8466298580169678\n",
            "Epoch: 0, Loss:  2.9160592555999756\n",
            "Epoch: 0, Loss:  3.349177837371826\n",
            "Epoch: 0, Loss:  2.5790867805480957\n",
            "Epoch: 0, Loss:  3.2401647567749023\n",
            "Epoch: 0, Loss:  2.3947854042053223\n",
            "Epoch: 0, Loss:  2.2626261711120605\n",
            "Epoch: 0, Loss:  2.6098427772521973\n",
            "Epoch: 0, Loss:  3.8146822452545166\n",
            "Epoch: 0, Loss:  2.4320099353790283\n",
            "Epoch: 0, Loss:  2.3282997608184814\n",
            "Epoch: 0, Loss:  2.0354394912719727\n",
            "Epoch: 0, Loss:  2.7363333702087402\n",
            "Epoch: 0, Loss:  2.1360528469085693\n",
            "Epoch: 0, Loss:  2.1020212173461914\n",
            "Epoch: 0, Loss:  1.9237146377563477\n",
            "Epoch: 0, Loss:  2.314549684524536\n",
            "Epoch: 0, Loss:  2.593585968017578\n",
            "Epoch: 0, Loss:  1.7425488233566284\n",
            "Epoch: 0, Loss:  2.0907280445098877\n",
            "Epoch: 0, Loss:  2.9126386642456055\n",
            "Epoch: 0, Loss:  2.380068302154541\n",
            "Epoch: 0, Loss:  1.5600658655166626\n",
            "Epoch: 0, Loss:  2.7503421306610107\n",
            "Epoch: 0, Loss:  2.4144558906555176\n",
            "Epoch: 0, Loss:  1.835336685180664\n",
            "Epoch: 0, Loss:  2.6617188453674316\n",
            "Epoch: 0, Loss:  1.7790745496749878\n",
            "Epoch: 0, Loss:  2.992250442504883\n",
            "Epoch: 0, Loss:  2.4951908588409424\n",
            "Epoch: 0, Loss:  2.3279647827148438\n",
            "Epoch: 0, Loss:  2.1885769367218018\n",
            "Epoch: 0, Loss:  2.168851613998413\n",
            "Epoch: 0, Loss:  2.227477788925171\n",
            "Epoch: 0, Loss:  2.5471253395080566\n",
            "Epoch: 0, Loss:  2.3865413665771484\n",
            "Epoch: 0, Loss:  3.0450279712677\n",
            "Epoch: 0, Loss:  1.6504395008087158\n",
            "Epoch: 0, Loss:  2.1843173503875732\n",
            "Epoch: 0, Loss:  3.0621707439422607\n",
            "Epoch: 0, Loss:  2.5232059955596924\n",
            "Epoch: 0, Loss:  1.841986894607544\n",
            "Epoch: 1, Loss:  1.9878939390182495\n",
            "Epoch: 1, Loss:  2.06290864944458\n",
            "Epoch: 1, Loss:  1.8019503355026245\n",
            "Epoch: 1, Loss:  2.568770170211792\n",
            "Epoch: 1, Loss:  1.4173192977905273\n",
            "Epoch: 1, Loss:  2.036372184753418\n",
            "Epoch: 1, Loss:  1.9866182804107666\n",
            "Epoch: 1, Loss:  0.9214527606964111\n",
            "Epoch: 1, Loss:  2.2107326984405518\n",
            "Epoch: 1, Loss:  2.592919111251831\n",
            "Epoch: 1, Loss:  1.7491248846054077\n",
            "Epoch: 1, Loss:  1.553589105606079\n",
            "Epoch: 1, Loss:  3.95869779586792\n",
            "Epoch: 1, Loss:  2.349015235900879\n",
            "Epoch: 1, Loss:  2.3657407760620117\n",
            "Epoch: 1, Loss:  2.587885618209839\n",
            "Epoch: 1, Loss:  2.075537919998169\n",
            "Epoch: 1, Loss:  1.7196122407913208\n",
            "Epoch: 1, Loss:  1.3947464227676392\n",
            "Epoch: 1, Loss:  2.1677424907684326\n",
            "Epoch: 1, Loss:  2.212012767791748\n",
            "Epoch: 1, Loss:  1.9599016904830933\n",
            "Epoch: 1, Loss:  2.106415271759033\n",
            "Epoch: 1, Loss:  1.7167104482650757\n",
            "Epoch: 1, Loss:  1.3303207159042358\n",
            "Epoch: 1, Loss:  2.040836811065674\n",
            "Epoch: 1, Loss:  2.4860169887542725\n",
            "Epoch: 1, Loss:  2.536212205886841\n",
            "Epoch: 1, Loss:  1.9523330926895142\n",
            "Epoch: 1, Loss:  1.2077066898345947\n",
            "Epoch: 1, Loss:  2.2085490226745605\n",
            "Epoch: 1, Loss:  2.321263074874878\n",
            "Epoch: 1, Loss:  1.8274288177490234\n",
            "Epoch: 1, Loss:  1.8780454397201538\n",
            "Epoch: 1, Loss:  2.2876975536346436\n",
            "Epoch: 1, Loss:  2.214492082595825\n",
            "Epoch: 1, Loss:  1.9110488891601562\n",
            "Epoch: 1, Loss:  2.2549004554748535\n",
            "Epoch: 1, Loss:  1.6323193311691284\n",
            "Epoch: 1, Loss:  1.6014704704284668\n",
            "Epoch: 1, Loss:  2.0282809734344482\n",
            "Epoch: 1, Loss:  2.8429770469665527\n",
            "Epoch: 1, Loss:  2.5300276279449463\n",
            "Epoch: 1, Loss:  1.9071351289749146\n",
            "Epoch: 1, Loss:  1.688873052597046\n",
            "Epoch: 1, Loss:  1.9141894578933716\n",
            "Epoch: 1, Loss:  0.6108404994010925\n",
            "Epoch: 1, Loss:  2.4768319129943848\n",
            "Epoch: 1, Loss:  1.569440245628357\n",
            "Epoch: 1, Loss:  2.17059588432312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TLe6RBH15x_",
        "colab_type": "text"
      },
      "source": [
        "### Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWYrKkyEOoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d504766c-48e7-4ed7-b636-521b3f8b4717"
      },
      "source": [
        "predictions, references = validate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 batches complete\n",
            "100 batches complete\n",
            "200 batches complete\n",
            "300 batches complete\n",
            "400 batches complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNSkCArN17ql",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKtXLkPVufwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9075d44f-4ca7-4c5a-a1e6-33aacf1f11dc"
      },
      "source": [
        "score = rouge_metric.compute(predictions, references)\n",
        "print(f\"Rouge Score: {score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge Score: {'rouge1': AggregateScore(low=Score(precision=0.28138156100908157, recall=0.3527661847311623, fmeasure=0.3055916009714199), mid=Score(precision=0.29002877421532913, recall=0.36236129551046076, fmeasure=0.3138579771459466), high=Score(precision=0.2985856825164749, recall=0.3718726211589361, fmeasure=0.32247776133352224)), 'rougeL': AggregateScore(low=Score(precision=0.19791089848290735, recall=0.24858328447546973, fmeasure=0.21483102637136084), mid=Score(precision=0.20549013202888677, recall=0.25776862848097515, fmeasure=0.22256389534739437), high=Score(precision=0.21410270286952704, recall=0.2664930538358284, fmeasure=0.23056950724572037))}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuayLxvR1-dh",
        "colab_type": "text"
      },
      "source": [
        "### Some Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4RRIe1Rffl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f50cb69-9003-4256-974d-1470fc29ff5f"
      },
      "source": [
        "for i in range(10):\n",
        "  print(f\"Prediction: {predictions[i]}\")\n",
        "  print(f\"Reference: {references[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  China has declared the Internet to be the new battlefield in its fight against \"pornography and unlawful information\"\n",
            "Su Changlan was detained solely for expressing peaceful views online.\n",
            "The crackdown is part of the worst crackdown against freedom of expression in China in more than a decade.\n",
            "Reference: China's Internet model is one of extreme control, says Amnesty's East Asia director.\n",
            "Chinese authorities suppress online debate on a range of legitimate issues, she says.\n",
            "While the battlefield is virtual, the impact on people's lives is real and devastating, she adds.\n",
            "Prediction:  man found hanging from tree in Mississippi woods with bedsheets around his neck.\n",
            "Authorities say they still have a lot of work ahead to figure out how Byrd died.\n",
            "\"The community deserves answers,\" sheriff says.\n",
            "Reference: Law enforcement officials say evidence collected so far doesn't suggest foul play.\n",
            "Forensics expert talks about how evidence differs in suicides and lynchings.\n",
            "FBI agent says a report on the cause of death is expected next week.\n",
            "Prediction:  page includes the show Transcript.\n",
            "Use the Transcript to help students with reading comprehension and vocabulary.\n",
            "At the bottom of the page, comment for a chance to be mentioned on CNN Student News. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "Reference: This page includes the show Transcript.\n",
            "Use the Transcript to help students with reading comprehension and vocabulary.\n",
            "At the bottom of the page, comment for a chance to be mentioned on CNN Student News.  You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "Prediction:  sheTalksYVR Conference is jam-packed with inspiring women from all walks of life.\n",
            "Inaugural event features 15 women speakers from the west coast of Canada with a diverse array of backgrounds.\n",
            "The focus is on male violence against women and speakers.\n",
            "Reference: Each year, people all over the world organize events around Intl. Women's Day.\n",
            "CNN picks the coolest conferences, gigs, walks and other events.\n",
            "Includes: Breakfast with scientists in New Zealand, mother-daughter coding in Luxembourg.\n",
            "Prediction: Kerry Washington is set to play Anita Hill and executive produce a TV movie for HBO.\n",
            "The telepic will detail the 1991 Clarence Thomas-Hill Supreme Court nomination hearings.\n",
            "\"Confirmation\" is being produced by Groundswell Productions.\n",
            "Reference: \"Scandal\" actress Kerry Washington to star as Anita Hill in HBO movie.\n",
            "Hill is the former colleague of Supreme Court Justice Clarence Thomas who accused him of sexual harassment.\n",
            "Prediction:  Malawi has one of the world's highest rates of child marriage by the U.N. Population Fund.\n",
            "It is ranked eighth of the 20 countries that are considered to have the highest rate of child marriage.\n",
            "The Constitution allows marriage below the age of 18, which prohibits child marriage.\n",
            "Reference: 50% of girls in Malawi will be married by the time they turn 18.\n",
            "Studies show the practice increases the risks of childbirth for both mother and infant.\n",
            "New laws preventing child marriage are still overruled by Malawi's Constitution.\n",
            "Prediction:  is the Middle East better off under dictatorships since the start of the Arab Spring in 2011.\n",
            "It is also a direct result of the West's own stance towards dictatorships in the region prior to and during the Arab Spring.\n",
            "Reference: Khatib: West supported Mideast dictators until it became unpalatable in 2011.\n",
            "Khatib: West's lack of a plan for aftermath of dictatorships has been catastrophic.\n",
            "Prediction:  state's ban on same-sex marriage is now against the law in fewer than one in four states.\n",
            "Nebraska state officials immediately appealed the ruling to the 8th Circuit U.S. Court of Appeals.\n",
            "The Supreme Court has ruled that it will not rule on the case until after the ruling.\n",
            "Reference: 12 states do not allow same-sex marriages.\n",
            "The U.S. Supreme Court could settle the issue for good when it rules this year.\n",
            "Prediction:  bank robbers are suspected in a string of robberies across Maryland and Virginia.\n",
            "The FBI is asking anyone with information on the robberies in Virginia to call 202-278-2000.\n",
            "They've earned their name for the black hats they wear during their heists.\n",
            "Reference: Latest of 8 robberies came at Wells Fargo bank in Falls Church, Virginia, during lunch hour.\n",
            "FBI: One robber white, the other black; they're in their 40s and stand about 5'8\"\n",
            "Trio includes a driver who waits outside while other two take over bank, FBI says.\n",
            "Prediction:  Kerry denounces GOP letter to Iran leaders.\n",
            "He says the deal is a \"nonbinding agreement\" that could jeopardize the long-term survival of any Iran deal.\n",
            "The White House defends the distinction as perfectly sound and no surprise.\n",
            "Reference: White House, many Republicans disagree over deal terminology, requirements.\n",
            "At some point, Congress will weigh in on some aspect of a prospective agreement.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJyHTP3mNImP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fdbb13f-a4f5-46aa-baa4-691216041197"
      },
      "source": [
        "model.eval()\n",
        "from pprint import pprint\n",
        "for i in range(10):\n",
        "  print(f\"{i}th sample\")\n",
        "  art = dataset_valid[i]['article']\n",
        "  summ = dataset_valid[i]['highlights']\n",
        "  source = tokenizer.batch_encode_plus([art], max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt', truncation=True)\n",
        "  target = tokenizer.batch_encode_plus([summ], max_length=MAX_LEN, pad_to_max_length=True,return_tensors='pt', truncation=True)\n",
        "  source_ids = source['input_ids']\n",
        "  source_mask = source['attention_mask']\n",
        "  target_ids = target['input_ids']\n",
        "  target_mask = target['attention_mask']\n",
        "\n",
        "  y = target_ids.to(device, dtype = torch.long)\n",
        "  ids = source_ids.to(device, dtype = torch.long)\n",
        "  mask = source_mask.to(device, dtype = torch.long)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "      input_ids = ids,\n",
        "      attention_mask = mask, \n",
        "      max_length=150, \n",
        "      num_beams=4,\n",
        "      repetition_penalty=2.5,\n",
        "      early_stopping=True\n",
        "      )\n",
        "  pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "  ref = [tokenizer.decode(r, skip_special_tokens=True, clean_up_tokenization_spaces=True)for r in y]\n",
        "  pprint(f\"Prediction: {pred}\")\n",
        "  pprint(f\"Reference: {ref}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0th sample\n",
            "(\"Prediction: [' singer-songwriter hits jogger with his car.\\\\nThe accident \"\n",
            " 'happened in Santa Ynez, California.\\\\nHe was driving at approximately 50 mph '\n",
            " 'when he hit the jogger.\\\\nHis injuries are not believed to be life '\n",
            " \"threatening.']\")\n",
            "(\"Reference: ['Accident happens in Santa Ynez, California, near where Crosby \"\n",
            " 'lives.\\\\nThe jogger suffered multiple fractures; his injuries are not '\n",
            " \"believed to be life-threatening.']\")\n",
            "1th sample\n",
            "(\"Prediction: [' fraternity was founded March 9, 1856, five years before the \"\n",
            " 'American Civil War.\\\\nThe group now boasts more than 200,000 living '\n",
            " 'alumni.\\\\nYale University banned SAEs from campus activities last month '\n",
            " \"after a string of member deaths.']\")\n",
            "('Reference: [\"Sigma Alpha Epsilon is being tossed out by the University of '\n",
            " \"Oklahoma.\\\\nIt's also run afoul of officials at Yale, Stanford and Johns \"\n",
            " 'Hopkins in recent months.\"]')\n",
            "2th sample\n",
            "('Prediction: [\\' of the \"Finding Jesus\" episode, Candida Moss is one of many '\n",
            " 'on-camera experts in CNN\\\\\\'s \"Finding Jesus\" series.\\\\nMoss: I\\\\\\'m so glad '\n",
            " 'someone brought up DNA,\" not the more reliable nuclear DNA.\\\\nShe says '\n",
            " \"it\\\\'s rare to find this kind of evidence outside of the writings of their \"\n",
            " \"followers.']\")\n",
            "(\"Reference: ['Religion professor Candida Moss appears in each episode of the \"\n",
            " 'program.\\\\nMoss was part of the original study to determine if relics found '\n",
            " \"in Bulgaria could be the bones of John the Baptist.']\")\n",
            "3th sample\n",
            "('Prediction: [\"erguson is illustrative of a broader problem across the '\n",
            " 'country as increasingly militarized majority-white police departments '\n",
            " \"demonstrate consistent racial bias.\\\\nMichael Brown's killing of Michael \"\n",
            " 'Brown last year highlighted a predatory policing problem.\\\\nHe says reforms '\n",
            " 'are needed if we want to reverse rising anger in Ferguson and elsewhere.\"]')\n",
            "(\"Reference: ['Two police officers were shot Wednesday in Ferguson.\\\\nHank \"\n",
            " \"Johnson, Michael Shank: Policing style needs rethink.']\")\n",
            "4th sample\n",
            "(\"Prediction: [' Bill Clinton defended his family foundation\\\\'s practice of \"\n",
            " 'taking money from foreign countries.\\\\nClinton was secretary of state when '\n",
            " 'Hillary Clinton became secretary of state in 2009.\\\\nClinton says the '\n",
            " 'foundation has done \"a lot more good than harm\"\\']')\n",
            "(\"Reference: ['Clinton Foundation has taken money from foreign \"\n",
            " 'governments.\\\\nBill Clinton:  \"I believe we have done a lot more good than '\n",
            " 'harm\"\\']')\n",
            "5th sample\n",
            "('Prediction: [\" the San Francisco-based startup took Austin by storm last '\n",
            " \"week.\\\\nJimmy Fallon is streaming his life using Meerkat.\\\\nCNNMoney's \"\n",
            " 'Laurie Segall reached out to Meerkat founder Ben Rubin with some '\n",
            " 'questions.\"]')\n",
            "(\"Reference: ['Join Meerkat founder Ben Rubin for a live chat at 2 p.m. ET \"\n",
            " 'Wednesday.\\\\nFollow @benrbn and @lauriesegallcnn on Meerkat.\\\\nUse hashtag '\n",
            " \"#CNNInstantStartups to join the conversation on Twitter.']\")\n",
            "6th sample\n",
            "(\"Prediction: [' U.S. diplomats have been under attack in the last few \"\n",
            " 'decades.\\\\nJohn Gordon Mein was shot and fell to the ground 12 yards behind '\n",
            " 'his limousine.\\\\nThe first U.S. ambassador assassinated while in office was '\n",
            " \"John Gordon Mein.']\")\n",
            "(\"Reference: ['Several U.S. diplomats have died after being attacked.\\\\nThey \"\n",
            " \"include then-Ambassadors Christopher Stevens, John Mein and Francis Meloy.']\")\n",
            "7th sample\n",
            "(\"Prediction: ['Debra Milke is only the second woman exonerated from death row \"\n",
            " 'in the United States.\\\\nShe was convicted of conspiring with two other men '\n",
            " 'to kill her son.\\\\nThe judge dismissed all charges against her after years '\n",
            " \"of legal back-and-forth.']\")\n",
            "(\"Reference: ['Debra Milke was convicted of murder in her son\\\\'s death, given \"\n",
            " 'the death penalty.\\\\nThere was no evidence tying her to the crime, but a '\n",
            " 'detective said she confessed.\\\\nThis detective had a \"history of '\n",
            " 'misconduct,\" including lying under oath.\\']')\n",
            "8th sample\n",
            "(\"Prediction: [' of 24,000 confirmed, reportable and suspected cases have been \"\n",
            " 'in three countries.\\\\nThe vast majority of the more than 24,000 confirmed, '\n",
            " \"reportable and suspected cases have been in those countries.']\")\n",
            "(\"Reference: ['Spokesperson: Experts are investigating how the UK military \"\n",
            " 'health care worker got Ebola.\\\\nIt is being decided if the military worker '\n",
            " 'infected in Sierra Leone will return to England.\\\\nThere have been some '\n",
            " \"24,000 reported cases and 10,000 deaths in the latest Ebola outbreak.']\")\n",
            "9th sample\n",
            "(\"Prediction: [' forces are reportedly approaching the city from five \"\n",
            " 'directions.\\\\nThe operation is part of a wide-scale offensive to retake '\n",
            " 'Tikrit and Salahuddin province.\\\\nIt has highlighted the role played by '\n",
            " \"neighboring Iran in the fight against ISIS.']\")\n",
            "('Reference: [\"Iraqi forces make some progress as they seek to advance toward '\n",
            " \"Tikrit.\\\\nThe city, best known to Westerners as Saddam Hussein's birthplace, \"\n",
            " 'was taken by ISIS in June.\"]')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}